<div align="center">

<div style="text-align: center; margin: 0; padding: 0;">
    <img src="figures/caption.png" alt="PackTron Logo" style="width: 400px; height: auto; display: block; margin: 0 auto;"/>
</div>

### **ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒçš„é«˜æ•ˆæ•°æ®åŠ è½½å™¨**

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh.md)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.7+-orange.svg)](https://pytorch.org/)

**é›¶å¡«å…… | ç»†ç²’åº¦æ•°æ®é…ç½® | è®­ç»ƒæ•ˆç‡ â¬†ï¸â¬†ï¸ | é«˜æ•ˆé›†æˆ**

[å…³é”®ç‰¹æ€§](#-å…³é”®ç‰¹æ€§) â€¢ [ä¸ºä»€ä¹ˆé€‰æ‹© PackTron?](#-ä¸ºä»€ä¹ˆé€‰æ‹©-packtron) â€¢ [å¿«é€Ÿä¸Šæ‰‹](#-å¿«é€Ÿä¸Šæ‰‹) â€¢ [å®‰è£…](#-å®‰è£…) â€¢ [æ–‡æ¡£](#-æ–‡æ¡£)

</div>

---

## ğŸ¯ å…³é”®ç‰¹æ€§

### âœ¨ **é›¶å¡«å……æµªè´¹**

- **åºåˆ—æ— å¡«å……** - é€šè¿‡æ™ºèƒ½å¥å­æ‰“åŒ…è®©æ‰€æœ‰åºåˆ—éƒ½æ°å¥½ä¸º `sequence_length`
- **100% Token åˆ©ç”¨ç‡** - æ•°æ®é›†ä¸­æ¯ä¸ª token éƒ½å‚ä¸è®­ç»ƒ
- **å‡†ç¡®çš„ Token è®¡æ•°** - ç²¾ç¡®æŒæ¡æ¨¡å‹åœ¨è®­ç»ƒä¸­çœ‹åˆ°çš„ token æ•°é‡

<div style="margin-top: 20px; overflow: hidden;"> 
    <div style="float: left; width: 98%; text-align: center; padding: 0 1%;">
        <img src="figures/load.png" alt="diff load" style="max-width: 98%;"/>
</div>

### ğŸ¨ **ç»†ç²’åº¦æ•°æ®é…æ¯”æ§åˆ¶**

- **åˆ†é˜¶æ®µè®­ç»ƒæ•°æ®è®¾ç½®** - çµæ´»ç¼–æ’æ•°æ®é›†ä½¿ç”¨é¡ºåºä¸å æ¯”ï¼Œä¾‹å¦‚åœ¨ç‰¹å®šé˜¶æ®µæ’å…¥ä»£ç æˆ–æ•°å­¦æ•°æ®ä»¥æå‡æ¨¡å‹è´¨é‡
- **æå‡æ¨¡å‹è¡¨ç°** - è½»æ¾å®ç°å¼ºå¤§çš„æ•°æ®é…ç½®ç­–ç•¥ï¼Œæ”¹è¿›æ¨¡å‹æ”¶æ•›å’Œæœ€ç»ˆæ•ˆæœ
- **èšç„¦å­¦ä¹ ** - ç¡®ä¿æ¨¡å‹åœ¨éœ€è¦çš„æ—¶é—´ç‚¹æ¥è§¦åˆ°æœ€ç›¸å…³çš„ä¸“ä¸šæ•°æ®

<div style="margin-top: 20px; overflow: hidden;"> 
    <div style="float: left; width: 98%; text-align: center; padding: 0 1%;">
        <img src="figures/advance_function.png" alt="improvement" style="max-width: 80%;"/>
</div>

### âš¡ **ç”Ÿäº§çº§æ€§èƒ½**

- **äºŒè¿›åˆ¶å­˜å‚¨æ ¼å¼** - é¢„å…ˆåˆ†è¯çš„æ•°æ®ä»¥ `.bin`/`.idx` æ–‡ä»¶å­˜å‚¨ï¼Œæ”¯æŒå†…å­˜æ˜ å°„ I/O
- **C++ åŠ é€Ÿç´¢å¼•** - é€šè¿‡ä¼˜åŒ–çš„ C++ï¼ˆpybind11ï¼‰å¿«é€Ÿæ„å»ºæ ·æœ¬ç´¢å¼•
- **è‡ªåŠ¨å¤š GPU æ•°æ®åˆ†å‘** - æŒ‰ç…§ `world_size` è‡ªåŠ¨åˆ‡åˆ†æ•°æ®ï¼Œå‘Šåˆ«æ‰‹åŠ¨é…ç½®
- **æ™ºèƒ½ Epoch ç®¡ç†** - æ ¹æ® `train_iters`/`eval_iters` ä¸æ•°æ®è§„æ¨¡è‡ªåŠ¨è®¡ç®—å¹¶æ„å»ºæ‰€éœ€ epochï¼Œç¡®ä¿è®­ç»ƒæ°¸ä¸ä¸­æ–­

### ğŸ”§ **æ˜“äºé›†æˆ**

- **å…¼å®¹ `transformers`** - å¯æ— ç¼æ›¿æ¢ç°æœ‰è®­ç»ƒæµç¨‹ä¸­çš„ `load_dataset`
- **ç®€å• API** - ä¸‰æ­¥å³å¯ï¼šé¢„å¤„ç† â†’ é…ç½® â†’ åŠ è½½å™¨
- **è½»é‡ä¾èµ–** - åªå…³æ³¨æ ¸å¿ƒèƒ½åŠ›ï¼Œå®‰è£…ç®€å•

---

## ğŸ¤” ä¸ºä»€ä¹ˆé€‰æ‹© PackTron?

### ä¼ ç»Ÿæ•°æ®åŠ è½½çš„ç—›ç‚¹

å½“ä½ ç”¨ `datasets` çš„ `load_dataset` è®­ç»ƒå¤§æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šé‡åˆ°ï¼š

```python
# åŸºäº transformers çš„ä¼ ç»Ÿåšæ³•
from datasets import load_dataset

dataset = load_dataset("text", data_files="data.jsonl")
# âŒ éœ€è¦å¡«å……åˆ° batch_size Ã— sequence_length
# âŒ ä¸èƒ½è‡ªåŠ¨æ„å»º epoch
# âŒ éš¾ä»¥ç®¡ç†å¤šé˜¶æ®µæ•°æ®æµæ°´çº¿
# âŒ æ— æ³•è‡ªåŠ¨åœ¨å¤š GPU é—´åˆ†å‘æ•°æ®
```

**å¸¸è§é—®é¢˜ï¼š**

- ğŸš« **å¡«å……æµªè´¹**ï¼šå…¸å‹æ‰¹æ¬¡ä¸­ 30-50% çš„ token éƒ½æ˜¯å¡«å……
- ğŸš« **æŒ‡æ ‡ä¸å‡†ç¡®**ï¼štoken è®¡æ•°åŒ…å« padding tokenï¼Œéš¾ä»¥è¿½è¸ªçœŸå®è®­ç»ƒè¿›åº¦
- ğŸš« **å†…å­˜æ•ˆç‡ä½**ï¼špadding token å ç”¨æ˜¾å­˜å´ä¸æä¾›æœ‰æ•ˆå­¦ä¹ ä¿¡å·
- ğŸš« **æ©ç å¤æ‚**ï¼šè¿˜éœ€å¤„ç†å¡«å……ä»¤ç‰Œçš„æ³¨æ„åŠ›æ©ç 
- ğŸš« **å¤š GPU æ‰‹åŠ¨åˆ†å‘**ï¼šåˆ†å¸ƒå¼è®­ç»ƒæ—¶å¿…é¡»æ‰‹åŠ¨åˆ‡åˆ†æ•°æ®
- ğŸš« **æ•°æ®è€—å°½é£é™©**ï¼šè‹¥æ•°æ®ä¸è¶³ï¼Œè®­ç»ƒå¯èƒ½æå‰åœæ­¢,é€ æˆè®­ç»ƒå¤±è´¥
- ğŸš« **ç¼ºä¹ç²¾ç»†æ§åˆ¶**ï¼šéš¾ä»¥å®ç°ç»†ç²’åº¦çš„æ•°æ®æ§åˆ¶

### PackTron çš„è§£å†³æ–¹æ¡ˆ

PackTron å€Ÿé‰´ Megatron-LM çš„ **ä¸‰å±‚æ•°æ®åŠ è½½æ¶æ„**ï¼Œå½»åº•è§£å†³ä¸Šè¿°ç“¶é¢ˆï¼š

```python
# PackTron ä½¿ç”¨æ–¹å¼
from packtron import create_dataloader, PackTronConfig

config = PackTronConfig(
    train_iters=1000,  # æ•°æ®ä¸è¶³æ—¶è‡ªåŠ¨è®¡ç®— epoch
    eval_iters=100,
    train_curriculum=[0.3, 0, 0.2, 1, 0.5, 0]
    ...
)
train_loader, eval_loader = create_dataloader(
    tokenizer, config,
    rank=0,           # å½“å‰ GPU rank
    world_size=2      # GPU æ€»æ•° - æ•°æ®è‡ªåŠ¨åˆ†å‘ï¼
)
# âœ… æ‰€æœ‰åºåˆ—é•¿åº¦å‡ä¸º sequence_lengthï¼ˆæ— å¡«å……ï¼‰
# âœ… ä¿è¯è®­ç»ƒæ‰€éœ€æ•°æ®é‡ï¼ˆè‡ªåŠ¨ epoch è®¡ç®—ï¼‰
# âœ… çµæ´»é…ç½®ä¸åŒé˜¶æ®µæ•°æ®é…æ¯”
# âœ… è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼æ•°æ®åˆ†ç‰‡
```

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**

- âœ… **é›¶å¡«å……**ï¼šæ¯ä¸ªåºåˆ—æ°å¥½ä¸º `sequence_length`
- âœ… **çœŸå®æŒ‡æ ‡**ï¼šåªç»Ÿè®¡æœ‰æ•ˆä»¤ç‰Œ
- âœ… **æ˜¾å­˜å‹å¥½**ï¼šé¿å…å¡«å……å ç”¨ GPU å†…å­˜
- âœ… **ç®€æ´**ï¼šæ— éœ€ç¼–å†™å¡«å……æ©ç é€»è¾‘
- âœ… **è‡ªåŠ¨å¤š GPU æ”¯æŒ**ï¼šä»…éœ€ä¼ å…¥ `rank` ä¸ `world_size`
- âœ… **è®­ç»ƒä¸ä¸­æ–­**ï¼šè‡ªåŠ¨é‡å¤æ•°æ®ä»¥å®Œæˆå…¨éƒ¨ `train_iters`
- âœ… **é«˜åº¦çµæ´»çš„æ•°æ®é…ç½®**ï¼šè¿è¡Œæ—¶å³å¯è°ƒæ•´æ•°æ®é¡ºåºï¼Œæ— éœ€é‡å»º `.bin/.idx`

---

## ğŸ—ï¸ å·¥ä½œåŸç†

PackTron é‡‡ç”¨ Megatron-LM éªŒè¯è¿‡çš„ **ä¸‰å±‚æ•°æ®åŠ è½½æ¶æ„**ï¼š

### ç¬¬ä¸€å±‚ï¼šäºŒè¿›åˆ¶å­˜å‚¨ (`IndexedDataset`)

- å°†åŸå§‹æ–‡æœ¬é¢„å¤„ç†ä¸ºåˆ†è¯åçš„äºŒè¿›åˆ¶æ ¼å¼ï¼ˆ`.bin`/`.idx`ï¼‰
- ä½¿ç”¨å†…å­˜æ˜ å°„ I/O å®ç°é«˜æ•ˆéšæœºè®¿é—®
- æŒ‰ä»¤ç‰Œ ID å­˜å‚¨æ–‡æ¡£

### ç¬¬äºŒå±‚ï¼šå¥å­æ‰“åŒ… (`GPTDataset`)

- å°†å¤šä¸ªæ–‡æ¡£æ™ºèƒ½æ‰“åŒ…åˆ°å›ºå®šé•¿åº¦åºåˆ—
- å€ŸåŠ© C++ åŠ é€Ÿå¿«é€Ÿæ„å»ºæ ·æœ¬ç´¢å¼•
- å¤„ç†æ–‡æ¡£è¾¹ç•Œå¹¶ç¡®ä¿åºåˆ—å¯¹é½

### ç¬¬ä¸‰å±‚ï¼šæ•°æ®é›†æ··åˆ (`BlendedDataset`)

- æ”¯æŒæŒ‰è‡ªå®šä¹‰æƒé‡æ··åˆå¤šä¸ªæ•°æ®é›†
- ç®¡ç†è®­ç»ƒ/éªŒè¯åˆ’åˆ†
- ç¼“å­˜ç´¢å¼•ä»¥åŠ é€Ÿåç»­åŠ è½½

### æ‰“åŒ…ç®—æ³•

PackTron ä½¿ç”¨ **Megatron-LM æˆç†Ÿçš„å¥å­æ‰“åŒ…ç®—æ³•**ï¼ŒåŒæ—¶ç®€åŒ– Megatron ä¾èµ–ï¼Œå¸®åŠ©ç”¨æˆ·æ— éœ€æ¥è§¦å®Œæ•´æ¡†æ¶å³å¯ä½¿ç”¨ã€‚æµç¨‹åŒ…æ‹¬ï¼š

1. **æ–‡æ¡£ç´¢å¼•**ï¼šæŒ‰å¥åˆ’åˆ†æ–‡æ¡£å¹¶è¿›è¡Œåˆ†è¯
2. **åºåˆ—æ„å»º**ï¼šä¸æ–­è¿æ¥å¥å­ç›´è‡³è¾¾åˆ° `sequence_length`
3. **è¾¹ç•Œå¤„ç†**ï¼šå¿…è¦æ—¶åœ¨åºåˆ—é—´åˆ†å‰²æ–‡æ¡£ï¼Œå¹¶æ’å…¥ç‰¹æ®Šæ ‡è®°
4. **ç´¢å¼•æ„å»º**ï¼šC++ ä»£ç ç”Ÿæˆé«˜æ•ˆæŸ¥æ‰¾ç´¢å¼•ï¼Œä¾¿äºå¿«é€Ÿé‡‡æ ·

è¿™æ„å‘³ç€ï¼š

- æ¯ä¸ªåºåˆ—é•¿åº¦éƒ½ç›¸åŒ
- å®Œå…¨ä¸éœ€è¦å¡«å……
- å¦‚æœéœ€è¦å¯ä¿ç•™æ–‡æ¡£è¾¹ç•Œ

**PackTron çš„è´¡çŒ®**ï¼šæˆ‘ä»¬åœ¨ä¿ç•™æ ¸å¿ƒæ¶æ„çš„åŸºç¡€ä¸Šï¼Œå‰¥ç¦» Megatron ç›¸å…³ä¾èµ–ï¼Œæä¾›ç®€æ´æ˜“ç”¨çš„ APIï¼Œå¹¶é¢å¤–è®¾è®¡çµæ´»çš„æ•°æ®é…ç½®æœºåˆ¶ï¼Œä¸ transformers ç­‰æ¡†æ¶å®Œç¾èåˆã€‚

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

### ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†æ•°æ®

å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸º PackTron çš„äºŒè¿›åˆ¶æ ¼å¼ï¼š

```bash
packtron-preprocess \
    --input data.jsonl \
    --output-prefix data \
    --tokenizer-model t5-base \
    --workers 4 \
    --append-eod
```

ç”Ÿæˆçš„äºŒè¿›åˆ¶æ–‡ä»¶åŒ…æ‹¬ï¼š

- `data_text_document.bin` / `data_text_document.idx` - åˆ†è¯åçš„äºŒè¿›åˆ¶æ•°æ®

**è¯´æ˜**ï¼šè¾“å‡ºæ ¼å¼ä¸º `{output_prefix}_{json_key}_{level}.bin/idx`ã€‚é»˜è®¤ `json_key` ä¸º `"text"`ï¼Œ`level` ä¸º `"document"`ï¼ˆå¦‚æœä½¿ç”¨ `--split-sentences` åˆ™ä¸º `"sentence"`ï¼‰ã€‚

### ç¬¬äºŒæ­¥ï¼šé…ç½® PackTron

åˆ›å»º `PackTronConfig`ï¼Œå¡«å†™è®­ç»ƒå‚æ•°ï¼š

```python
from packtron import PackTronConfig

config = PackTronConfig(
    path_to_cache="./cache",          # ç´¢å¼•ç¼“å­˜ç›®å½•
    split_config="98,2",              # è®­ç»ƒ:98%ï¼ŒéªŒè¯:2%
    data_path="data",                 # .bin/.idx æ–‡ä»¶å‰ç¼€è·¯å¾„
    train_iters=1000,                 # è®­ç»ƒè¿­ä»£æ¬¡æ•°
    eval_iters=100,                   # éªŒè¯è¿­ä»£æ¬¡æ•°
    sequence_length=4096,             # å›ºå®šåºåˆ—é•¿åº¦
    batch_size=4,                     # æ¯å— GPU çš„ batch size
    random_seed=42,                   # éšæœºç§å­
    train_curriculum="0.5 0 0.5 1"    # å¯é€‰ï¼šæ•°æ®ç»†ç²’åº¦é…æ¯” ï¼ˆå‚è§ç¬¬å››æ­¥ï¼‰
)
```

### ç¬¬ä¸‰æ­¥ï¼šåˆ›å»º DataLoader

ä½¿ç”¨ PackTron çš„ `create_dataloader`ï¼š

```python
from packtron import create_dataloader
from transformers import AutoTokenizer

# åˆå§‹åŒ–åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("gpt2")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ„å»º DataLoaderï¼ˆè‡ªåŠ¨å¤„ç†å¤š GPUï¼‰
train_loader, eval_loader = create_dataloader(
    tokenizer=tokenizer,
    config=config,
    rank=0,          # å½“å‰ GPU rank
    world_size=2,    # GPU æ€»æ•°
    consumed_samples=0
)

# å°†å…¶åº”ç”¨åœ¨è®­ç»ƒå¾ªç¯ä¸­
# PackTron è‡ªåŠ¨å¤„ç†ï¼š
# - æŒ‰ rank/world_size åˆ‡åˆ†æ•°æ®
# - è®¡ç®— epochï¼Œé‡å¤æ•°æ®ä»¥å®Œæˆ train_iters/eval_iters
for batch in train_loader:
    tokens = batch['tokens']        # å½¢çŠ¶ï¼š[batch_size, sequence_length]
    labels = batch['labels']        # å½¢çŠ¶ï¼š[batch_size, sequence_length]
    attention_mask = batch['attention_mask'].float()  # å½¢çŠ¶ï¼š[batch_size, sequence_length]

    # æ— éœ€å¡«å……ï¼æ‰€æœ‰åºåˆ—è‡ªåŠ¨æ‹¼æ¥
    outputs = model(input_ids=tokens, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss
```

**å°±æ˜¯è¿™ä¹ˆç®€å•ï¼** ä½ çš„æ•°æ®ç°åœ¨ä»¥é›¶å¡«å……æ–¹å¼é«˜æ•ˆæ‰“åŒ…ã€‚

### ç¬¬å››æ­¥ï¼ˆå¯é€‰ï¼‰ï¼šç¼–æ’è®­ç»ƒæ•°æ®é…ç½®

æƒ³åœ¨ç‰¹å®šé˜¶æ®µåˆ‡æ¢ç¼–ç ã€æ•°å­¦ã€ç‰©ç†ç­‰è¯­æ–™ï¼Ÿåœ¨é…ç½®ä¸­è®¾ç½® `train_curriculum`ï¼š

```python
config = PackTronConfig(
    # ...å…¶ä»–å­—æ®µ...
    data_path="0.3 coding_data 0.3 math_data 0.4 physics_data",
    train_curriculum="0.2 0 0.2 1 0.2 2 0.1 0 0.1 1 0.2 2"
)
```

åŸºäºä¸Šè¿° config çš„ data_path è®¾ç½®ï¼ŒPackTron è‡ªåŠ¨å°† 30% çš„æ•°æ®éœ€æ±‚åˆ†é…ç»™ coding æ•°æ®é›†ï¼Œè‹¥æ•°æ®æºä¸è¶³ä»¥æä¾›æ•°æ®éœ€æ±‚è¦æ±‚çš„æ•°æ®ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å°†å…¶æ‰“åŒ…ä¸ºå¤šä¸ª epochï¼Œç›´åˆ°è¶³å¤Ÿæ»¡è¶³éœ€æ±‚ã€‚math å’Œ physics æ•°æ®é›†äº¦éµå¾ªç›¸åŒçš„é€»è¾‘ã€‚

train_curriculum çš„è®¾ç½®ä¼šè¦æ±‚ PackTron å°†è®­ç»ƒæ•°æ®æµæŒ‰æ¯”ä¾‹é˜¶æ®µæ€§åˆ’åˆ†ï¼šå‰ 20% ä½¿ç”¨ coding (æ•°æ®é›† ID 0)ï¼Œéšå 20% ä½¿ç”¨ mathï¼Œæ¥ç€ 20% ä½¿ç”¨ physicsï¼Œä»¥æ­¤ç±»æ¨ã€‚

PackTron ä¼šå¤ç”¨ç¼“å­˜çš„ Megatron-LM æ··åˆç´¢å¼•ï¼Œåªéœ€åœ¨å†…å­˜ä¸­é‡æ–°æ’åº `dataset_index` / `dataset_sample_index`ï¼Œå³å¯åœ¨ä¸é‡æ–°åˆ†è¯æˆ–é‡å»º `.bin/.idx` çš„æƒ…å†µä¸‹åˆ‡æ¢æ•°æ®é˜¶æ®µã€‚

---

## ğŸ“¦ å®‰è£…

### å…ˆå†³æ¡ä»¶

- Python >= 3.8
- PyTorch >= 2.7.0
- C++ ç¼–è¯‘å™¨ï¼ˆg++ æˆ– clang++ï¼‰
- pybind11ï¼ˆç”¨äº C++ æ‰©å±•ï¼‰

### å®‰è£… PackTron

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/JimmyAwoe/PackTron.git
cd PackTron

# æ¨èæ–¹å¼ï¼šä½¿ç”¨ pip
pip install -e .

# è¯¥å‘½ä»¤ä¼šï¼š
# - å®‰è£…æ‰€æœ‰ä¾èµ–
# - è‡ªåŠ¨ç¼–è¯‘ C++ æ‰©å±•
```

### æ‰‹åŠ¨å®‰è£…

è‹¥è‡ªåŠ¨ç¼–è¯‘å¤±è´¥ï¼Œå¯æŒ‰ä»¥ä¸‹æ­¥éª¤ï¼š

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ‰‹åŠ¨ç¼–è¯‘ C++ æ‰©å±•
cd packtron/utils/
make
cd ../..
```

### éªŒè¯å®‰è£…

```bash
python -c "from packtron import create_dataloader, PackTronConfig; print('âœ“ Installation successful!')"
```

æ›´è¯¦ç»†çš„å®‰è£…è¯´æ˜è§ [INSTALL.md](INSTALL.md)ã€‚

---

## ğŸ“š æ–‡æ¡£

### å®Œæ•´ç¤ºä¾‹

å‚è§ `examples/llama_train.py`ï¼Œå…¶ä¸­åŒ…å«ï¼š

- ä½¿ç”¨ `torchrun` çš„å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒ
- é›†æˆ LLaMA æ¨¡å‹
- éªŒè¯å¾ªç¯
- æ£€æŸ¥ç‚¹ä¿å­˜
- é€šè¿‡ `--train-curriculum` å®ç°æ•°æ®ç»†ç²’åº¦é…ç½®

```bash
# ä½¿ç”¨ 2-GPU è¿è¡Œ
torchrun --nproc-per-node=2 examples/llama_train.py \
    --model-config llama_60m.json \
    --tokenizer-model t5-base \
    --data-path data \
    --cache-dir ./cache \
    --split-config 98,2 \
    --sequence-length 4096 \
    --batch-size 4 \
    --train-iters 1000 \
    --eval-iters 10 \
    --eval-interval 100 \
    --train-curriculum 0.3 0 0.3 1 0.2 0 0.2 1
```

æƒ³å¿«é€Ÿä½“éªŒï¼Ÿæ‰§è¡Œ `examples/run.sh` å³å¯å¤ç°ä¸Šè¿°å‘½ä»¤ï¼Œä¸€é”®å¯åŠ¨åˆ†é˜¶æ®µè®­ç»ƒæµç¨‹ã€‚

### æ•°æ®è°ƒåº¦æ·±å…¥è§£æ

`train_curriculum` æ¥å—ä¸€ç³»åˆ— `<å æ¯”> <dataset_id>` å¯¹ã€‚PackTron ä¼šå°†å æ¯”å½’ä¸€åŒ–ï¼Œå¹¶æŒ‰é¡ºåºä»æŒ‡å®šæ•°æ®é›†ä¸­æŠ½æ ·ï¼Œ**æ— éœ€ä¿®æ”¹ç¼“å­˜ç´¢å¼•**ã€‚ä¾‹å¦‚ï¼š

```
train_curriculum="0.3 0 0.3 1 0.2 0 0.2 1"
```

è§£é‡Šå¦‚ä¸‹ï¼š

- å‰ 30% è®­ç»ƒæ­¥éª¤ä½¿ç”¨æ•°æ®é›† `0`
- æ¥ä¸‹æ¥çš„ 30% ä½¿ç”¨æ•°æ®é›† `1`
- éšå 20% å†å›åˆ°æ•°æ®é›† `0`
- æœ€å 20% ä½¿ç”¨æ•°æ®é›† `1` å®Œæˆè®­ç»ƒ

ä¸ Hugging Face `load_dataset` ä¸åŒï¼ŒPackTron æ— éœ€æ‰‹åŠ¨æ‰“ä¹±æˆ–é‡å¤é¢„å¤„ç†ï¼›æ•°æ®è°ƒæ•´å¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¬æ—¶ç”Ÿæ•ˆã€‚

### API å‚è€ƒ

#### `PackTronConfig`

PackTron æ•°æ®é›†çš„é…ç½®å¯¹è±¡ï¼š

```python
@dataclass
class PackTronConfig:
    path_to_cache: str          # æ•°æ®é›†ç´¢å¼•ç¼“å­˜ç›®å½•
    split_config: str           # æ•°æ®åˆ’åˆ†æ¯”ä¾‹ï¼Œå¦‚ '98,2'
    data_path: str              # æ•°æ®è·¯å¾„å‰ç¼€
    train_iters: int            # è®­ç»ƒè¿­ä»£æ¬¡æ•°
    eval_iters: int             # éªŒè¯è¿­ä»£æ¬¡æ•°
    sequence_length: int        # è®­ç»ƒæ ·æœ¬åºåˆ—é•¿åº¦
    batch_size: int             # æ¯ GPU çš„ batch å¤§å°
    random_seed: int = 1234     # æ•°æ®é›†æ‰“ä¹±çš„éšæœºç§å­
    train_curriculum: Optional[str] = None # å¯é€‰çš„æ•°æ®è°ƒåº¦
```

#### `create_dataloader`

åˆ›å»º PackTron DataLoader çš„å·¥å‚å‡½æ•°ï¼š

```python
from packtron import create_dataloader

def create_dataloader(
    tokenizer,                  # åˆ†è¯å™¨å®ä¾‹
    config: PackTronConfig,     # PackTron é…ç½®
    rank: int = 0,              # å½“å‰è¿›ç¨‹ rank
    world_size: int = 1,        # æ€»è¿›ç¨‹æ•°
    consumed_samples: int = 0   # å·²æ¶ˆè€—æ ·æœ¬æ•°ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
) -> Tuple[DataLoader, Optional[DataLoader]]:
    """è¿”å› (train_dataloader, eval_dataloader)"""
```

#### `build_tokenizer`

æ„å»ºåˆ†è¯å™¨çš„å·¥å…·å‡½æ•°ï¼š

```python
from packtron import build_tokenizer

tokenizer = build_tokenizer(args)  # args éœ€åŒ…å« tokenizer_model
```

å†…éƒ¨è°ƒç”¨å¦‚ä¸‹ï¼š

```python
import transformers

transformers.AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=args.tokenizer_model, **kwargs
        )
```

å› æ­¤å¯ä½¿ç”¨ä»»æ„ Hugging Face æ”¯æŒçš„åˆ†è¯å™¨ï¼Œç¡®ä¿ transformers ä¸ PackTron å®Œå…¨å…¼å®¹ã€‚

---

## ğŸ†š ä¸å…¶ä»–æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§                | PackTron            | `datasets.load_dataset` | Megatron-LM     |
| ------------------- | ------------------- | ----------------------- | --------------- |
| **é›¶å¡«å……**          | âœ… æ”¯æŒ             | âŒ ä¸æ”¯æŒ               | âœ… æ”¯æŒ         |
| **å‡†ç¡®ä»¤ç‰Œè®¡æ•°**    | âœ… æ”¯æŒ             | âŒ ä¸æ”¯æŒ               | âœ… æ”¯æŒ         |
| **æ˜“äºé›†æˆ**        | âœ… API ç®€æ´         | âœ… ç®€æ´                 | âŒ å¤æ‚         |
| **è½»é‡çº§**          | âœ… ä¾èµ–å°‘           | âœ… ä¾èµ–å°‘               | âŒ ä¾èµ–é‡       |
| **äºŒè¿›åˆ¶æ ¼å¼**      | âœ… å¿«é€Ÿ I/O         | âŒ æ–‡æœ¬æ ¼å¼             | âœ… å¿«é€Ÿ I/O     |
| **C++ åŠ é€Ÿ**        | âœ… æ”¯æŒ             | âŒ ä¸æ”¯æŒ               | âœ… æ”¯æŒ         |
| **è‡ªåŠ¨å¤š GPU åˆ†å‘** | âœ… è‡ªåŠ¨             | âŒ æ‰‹åŠ¨                 | âœ… æ”¯æŒ         |
| **è‡ªåŠ¨ Epoch ç®¡ç†** | âœ… æ”¯æŒ             | âŒ ä¸æ”¯æŒ               | âœ… æ”¯æŒ         |
| **çµæ´»æ•°æ®é…ç½®**    | âœ… é‡æ–°æ’åºæ— éœ€é‡å»º | âŒ ä¸æ”¯æŒ               | âš ï¸ éœ€è‡ªå®šä¹‰è„šæœ¬ |

---

## ğŸ“ é€‚ç”¨åœºæ™¯

PackTron éå¸¸é€‚åˆï¼š

- ğŸ§ª **ç§‘ç ”**ï¼šç²¾ç¡®çš„ä»¤ç‰Œè®¡æ•°ï¼Œä¾¿äºå¯å¤ç°å®éªŒ
- ğŸ­ **ç”Ÿäº§è®­ç»ƒ**ï¼šå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„é«˜æ•ˆæ•°æ®åŠ è½½
- ğŸ“Š **æ•°æ®åˆ†æ**ï¼šå‡†ç¡®äº†è§£æ¨¡å‹æ¥è§¦åˆ°çš„ä»¤ç‰Œ
- ğŸš€ **èµ„æºå—é™è®­ç»ƒ**ï¼šé€šè¿‡é›¶å¡«å……æµªè´¹æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡

---

## ğŸ¤ å‚ä¸è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼å¯ç›´æ¥æäº¤ Pull Requestã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ [LICENSE](LICENSE)ã€‚

æ³¨æ„ï¼šé¡¹ç›®åŒ…å«æ¥æºäº Megatron-LMï¼ˆNVIDIAï¼‰çš„ä»£ç ï¼Œå…¶è®¸å¯è¯ä¸º BSD-3-Clauseã€‚å®Œæ•´ä¿¡æ¯å‚è§ [LICENSE](LICENSE)ã€‚

---

## ğŸ™ è‡´è°¢

- **Megatron-LM** - ä¼˜ç§€çš„æ•°æ®åŠ è½½æ¶æ„
- **transformers** - åˆ†è¯å™¨é›†æˆæ”¯æŒ
- **Facebook Fairseq** - IndexedDataset åŸå§‹å®ç°

---

## ğŸ“§ è”ç³»æ–¹å¼

- **ä½œè€…**ï¼šJimmyAwoe
- **GitHub**ï¼š[@JimmyAwoe](https://github.com/JimmyAwoe)
- **é—®é¢˜åé¦ˆ**ï¼š[GitHub Issues](https://github.com/JimmyAwoe/PackTron/issues)

---

<div align="center">

**Made with â¤ï¸ for the LLM community**

â­ å¦‚æœä½ è§‰å¾—æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ä¸ºä»“åº“ç‚¹ä¸ª Starï¼

</div>
